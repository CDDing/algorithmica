---
title: Newton's Method
weight: 3
---

실용적인 알고리즘에서 가능한 최대 정밀도에 도달해야 하는 경우는 거의 없습니다. 실제 데이터에서는 모델링 오차나 측정 오차가 부동 소수점 반올림 등에서 발생하는 오차보다 훨씬 큰 경우가 대부분이기 때문입니다. 그래서 종종 정밀도를 조금 희생하더라도 속도를 얻을 수 있는 근사적인 방법을 택하는 것에 만족하기도 합니다.

여기서는 이러한 근사 수치 알고리즘에서 가장 중요한 구성 요소중 하나인 Newton 기법을 소개하겠습니다.

## Newton 기법

Newton 기법은 실수값 함수의 근을 근사적으로 찾기 위한 매우 간단하지만 강력한 알고리즘입니다. 즉, 다음과 같은 일반적인 방정식의 해를 구하는 것이 목표입니다.

$$
f(x) = 0
$$

함수 $f$에 대해 가정하는 것은 단 두 가지 입니다. 적어도 하나의 해가 존재하며, 검색 구간 내에서 $f(x)$가 연속이고 미분 가능하다는 것입니다. 물론 몇 가지 [특수한 예외 상황](https://en.wikipedia.org/wiki/Newton%27s_method#Failure_analysis)도 존재하지만 실제에서는 거의 발생하지 않으므로, 우리는 그냥 이 함수가 좋다(good)고 표현하겠습니다.

이 알고리즘의 핵심 아이디어는 초기 근사값 $x_0$에서 시작하여 반복적으로 개선해 나가는 것입니다. 각 단계에서는 $x = x_i$에서의 함수 그래프에 접선을 그린 뒤, 그 접선이 $x$축과 만나는 지점의 $x$ 좌표를 다음 근사값 $x_{i+1}$로 설정합니다. 직관적으로, 함수 $f$가 "[좋은](https://en.wikipedia.org/wiki/Smoothness)" 함수이고, $x_i$가 이미 근에 충분히 가까운 값이라면, $x_{i+1}$은 그보다 더 가까워질 것입니다.

![](../img/newton.png)

$x_n$에서의 접선이 $x$축과 교차하는 지점을 구하려면, 접선의 식을 0과 같게 만들어야 합니다.

$$
0 = f(x_i) + (x_{i+1} - x_i) f'(x_i)
$$

이로부터 다음과 같은 공식을 얻을 수 있습니다.

$$
x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}
$$

Newton 기법은 매우 중요한 기법으로, 과학 및 공학 분야에서 널리 사용되는 다양한 최적화 기법들의 기본이 되는 알고리즘입니다.

### 제곱근

간단한 예제로, 제곱근을 찾는 문제에 대해 이 알고리즘이 어떻게 적용되는지 유도해 봅시다.

$$
x = \sqrt n \iff x^2 = n \iff f(x) = x^2 - n = 0
$$

위의 일반적인 공식에 $f(x) = x^2 - n$을 대입하면, 다음과 같은 규칙을 얻을 수 있습니다.

$$
x_{i+1} = x_i - \frac{x_i^2 - n}{2 x_i} = \frac{x_i + n / x_i}{2}
$$

실제로는 정답에 충분히 가까워졌을 때 가능한 한 빨리 반복을 종료하고 싶습니다. 이는 각 반복 이후 오차를 간단히 검사함으로써 확인할 수 있습니다.

```cpp
const double EPS = 1e-9;

double sqrt(double n) {
    double x = 1;
    while (abs(x * x - n) > eps)
        x = (x + n / x) / 2;
    return x;
}
```

이 알고리즘은 많은 함수에서 수렴하지만, 항상 보장되고 증명가능한 것은 아니며, 일반적으로는 특정 조건을 만족하는 함수들에서만 확실하게 작동합니다. 또 다른 중요한 문제는, 수렴이 일어난다면 얼마나 빠르게 수렴하는지입니다.

### 수렴 비율

$2$의 제곱근을 구하기 위해 Newton 기법을 몇 차례 반복해 봅시다. 초기값 $x_0=1$에서 시작해서, 각 반복마다 얼마나 많은 자릿수가 정확해지는지 확인해보겠습니다.

<pre class='center-pre'>
<b>1</b>.0000000000000000000000000000000000000000000000000000000000000
<b>1</b>.5000000000000000000000000000000000000000000000000000000000000
<b>1.41</b>66666666666666666666666666666666666666666666666666666666675
<b>1.41421</b>56862745098039215686274509803921568627450980392156862745
<b>1.41421356237</b>46899106262955788901349101165596221157440445849057
<b>1.41421356237309504880168</b>96235025302436149819257761974284982890
<b>1.41421356237309504880168872420969807856967187537</b>72340015610125
<b>1.4142135623730950488016887242096980785696718753769480731766796</b>
</pre>

자세히 살펴보면, 반복할 때마다 정확한 자릿수가 거의 두 배로 증가함을 알 수 있습니다. 이 놀라운 수렴 속도는 단순한 우연이 아닙니다.

이 수렴 속도를 정량적으로 분석하기 위해, $i$번째 반복에서의 작은 상대 오차 $\delta_i$를 정의하고, 다음 반복에서의 오차 $\delta_{i+1}$이 얼마나 작아지는지를 살펴봅시다.

$$
|\delta_i| = \frac{|x_n - x|}{x}
$$

$x_i$를 $x \cdot (1 + \delta_i)$로 표현할 수 있습니다. 이를 Newton 반복 식에 대입하고 양변을 $x$로 나누면 다음과 같은 식을 얻을 수 있습니다.

$$
1 + \delta_{i+1} = \frac{1}{2} (1 + \delta_i + \frac{1}{1 + \delta_i}) = \frac{1}{2} (1 + \delta_i + 1 - \delta_i + \delta_i^2 + o(\delta_i^2)) = 1 + \frac{\delta_i^2}{2} + o(\delta_i^2)
$$

여기서는 $(1 + \delta_i)^{-1}$을 $0$ 근처에서 테일러 전개 했고, $d_i$가 매우 작다는 가정을 사용했습니다.(수열이 수렴하므로 충분히 큰 $i$에 대해 $d_i \ll 1$이 보장됩니다.)

$\delta_{i+1}$에 대해 정리하면 다음과 같은 식을 얻습니다.

$$
\delta_{i+1} = \frac{\delta_i^2}{2} + o(\delta_i^2)
$$

이는 해에 충분히 가까워졌을 때 반복할수록 오차가 대략 제곱되고 절반이 된다는 의미입니다. 로그 값 $(- \log_{10} \delta_i)$는 대략적으로 ${x_i}$의 유효 숫자 자릿수이므로, 상대 오차가 제곱된다는 것은 정확한 자릿수가 두 배로 늘어남을 의미합니다.

이러한 현상은 이차 수렴이라고 불리며, 사실 제곱근을 구할 때에만 나타나는 특성은 아닙니다. 세부적인증명은 독자에게 남기지만, 일반적으로 다음과 같은 식이 성립함을 보일 수 있습니다.

$$
|\delta_{i+1}| = \frac{|f''(x_i)|}{2 \cdot |f'(x_n)|} \cdot \delta_i^2
$$

이는 몇 가지 조건이 추가로 만족될 때 적어도 이차 수렴을 보장합니다. 그 조건은 $f'(x)$가 $0$이 아니고 $f''(x)$가 연속이라는 것입니다.

## 읽을거리

[Introduction to numerical methods at MIT](https://ocw.mit.edu/courses/mathematics/18-330-introduction-to-numerical-analysis-spring-2012/lecture-notes/MIT18_330S12_Chapter4.pdf).
